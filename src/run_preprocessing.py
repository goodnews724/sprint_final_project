"""
ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
íŒ€ì›ë³„ ë‹´ë‹¹ ì—…ë¬´ í†µí•© ì‹¤í–‰
"""

import sys
import os
import time
import logging
from datetime import datetime
import gc
import psutil

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from load_data import load_table
from preprocess_accounts_user import preprocess_accounts_user
from preprocess_hackle_events import preprocess_hackle_events
from preprocess_accounts_userquestionrecord import preprocess_userquestionrecord
from preprocess_accounts_blockrecord import preprocess_blockrecord
from save_data import save_to_gcs

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f'preprocessing_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)

def check_system_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬"""
    memory_percent = psutil.virtual_memory().percent
    cpu_percent = psutil.cpu_percent(interval=1)
    disk_percent = psutil.disk_usage('/tmp').percent
    
    logger.info(f"ğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì²´í¬:")
    logger.info(f"   ë©”ëª¨ë¦¬: {memory_percent:.1f}%")
    logger.info(f"   CPU: {cpu_percent:.1f}%")
    logger.info(f"   ë””ìŠ¤í¬(/tmp): {disk_percent:.1f}%")
    
    if memory_percent > 90:
        raise MemoryError(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìœ„í—˜ ìˆ˜ì¤€: {memory_percent:.1f}%")
    if disk_percent > 90:
        raise RuntimeError(f"ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {disk_percent:.1f}%")
    
    return {
        'memory': memory_percent,
        'cpu': cpu_percent,
        'disk': disk_percent
    }

def run_single_preprocessing(processor_name: str, table_name: str, dataset: str, preprocess_func):
    """ê°œë³„ ì „ì²˜ë¦¬ ì‹¤í–‰"""
    start_time = time.time()
    
    try:
        logger.info(f"\nğŸ”„ {processor_name}: {table_name} ì²˜ë¦¬ ì‹œì‘")
        
        # ë¦¬ì†ŒìŠ¤ ì²´í¬
        resources_before = check_system_resources()
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = load_table(table_name, dataset)
        original_count = len(df)
        
        # 2. ì „ì²˜ë¦¬
        df_clean = preprocess_func(df)
        processed_count = len(df_clean)
        
        # 3. ì €ì¥
        result = save_to_gcs(df_clean, table_name, 'processed')
        
        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬
        del df_clean
        gc.collect()
        
        # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        elapsed_time = time.time() - start_time
        
        # ê²°ê³¼ ìš”ì•½
        summary = {
            'processor': processor_name,
            'table_name': table_name,
            'original_rows': original_count,
            'processed_rows': processed_count,
            'processing_time_seconds': round(elapsed_time, 2),
            'status': 'SUCCESS',
            'gcs_info': result
        }
        
        logger.info(f"âœ… {processor_name}: {table_name} ì™„ë£Œ")
        logger.info(f"   ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        logger.info(f"   ë°ì´í„°: {original_count:,}í–‰ â†’ {processed_count:,}í–‰")
        
        return summary
        
    except Exception as e:
        elapsed_time = time.time() - start_time
        logger.error(f"âŒ {processor_name}: {table_name} ì‹¤íŒ¨ - {str(e)}")
        
        return {
            'processor': processor_name,
            'table_name': table_name,
            'processing_time_seconds': round(elapsed_time, 2),
            'status': 'FAILED',
            'error': str(e)
        }

def run_all_preprocessing(parallel: bool = False):
    """ëª¨ë“  í…Œì´ë¸” ì „ì²˜ë¦¬ ì‹¤í–‰"""
    
    pipeline_start_time = time.time()
    
    logger.info("ğŸš€ ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ì²´í¬
    try:
        initial_resources = check_system_resources()
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ë‹¨: {e}")
        return
    
    # ì „ì²˜ë¦¬ ì‘ì—… ì •ì˜
    preprocessing_tasks = [
        {
            'processor': 'ì²œì§€í˜„',
            'table_name': 'accounts_user',
            'dataset': 'votes',
            'function': preprocess_accounts_user
        },
        {
            'processor': 'ì¡°ìˆ˜ì§„',
            'table_name': 'hackle_events',
            'dataset': 'hackle',
            'function': preprocess_hackle_events
        },
        {
            'processor': 'ì§„ìš°í˜•',
            'table_name': 'accounts_userquestionrecord',
            'dataset': 'votes',
            'function': preprocess_userquestionrecord
        },
        {
            'processor': 'ì´ì¤€í¬',
            'table_name': 'accounts_blockrecord',
            'dataset': 'votes',
            'function': preprocess_blockrecord
        }
    ]
    
    results = []
    
    if parallel:
        # ë³‘ë ¬ ì²˜ë¦¬ (ì„ íƒì‚¬í•­) - ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•  ë•Œë§Œ
        logger.info("âš¡ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ")
        from concurrent.futures import ThreadPoolExecutor
        
        with ThreadPoolExecutor(max_workers=2) as executor:  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ 2ê°œë§Œ
            futures = []
            for task in preprocessing_tasks:
                future = executor.submit(
                    run_single_preprocessing,
                    task['processor'],
                    task['table_name'],
                    task['dataset'],
                    task['function']
                )
                futures.append(future)
            
            for future in futures:
                result = future.result()
                results.append(result)
    else:
        # ìˆœì°¨ ì²˜ë¦¬ (ê¸°ë³¸) - ë©”ëª¨ë¦¬ ì•ˆì „
        logger.info("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
        for i, task in enumerate(preprocessing_tasks, 1):
            logger.info(f"\nğŸ“ ì§„í–‰ë¥ : {i}/{len(preprocessing_tasks)}")
            
            result = run_single_preprocessing(
                task['processor'],
                task['table_name'],
                task['dataset'],
                task['function']
            )
            results.append(result)
            
            # ì¤‘ê°„ ë¦¬ì†ŒìŠ¤ ì²´í¬ ë° ì •ë¦¬
            gc.collect()
            time.sleep(1)  # ì‹œìŠ¤í…œ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ
    pipeline_elapsed_time = time.time() - pipeline_start_time
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‰ ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info("=" * 80)
    
    # ê²°ê³¼ ìš”ì•½
    success_count = sum(1 for r in results if r['status'] == 'SUCCESS')
    failed_count = len(results) - success_count
    
    logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
    logger.info(f"   âœ… ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"   âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"   â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {pipeline_elapsed_time:.1f}ì´ˆ")
    
    # ê°œë³„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
    for result in results:
        if result['status'] == 'SUCCESS':
            logger.info(f"   âœ… {result['table_name']} ({result['processor']})")
            logger.info(f"      {result['original_rows']:,}í–‰ â†’ {result['processed_rows']:,}í–‰")
            logger.info(f"      ì²˜ë¦¬ ì‹œê°„: {result['processing_time_seconds']}ì´ˆ")
        else:
            logger.info(f"   âŒ {result['table_name']} ({result['processor']})")
            logger.info(f"      ì˜¤ë¥˜: {result['error']}")
    
    if success_count > 0:
        logger.info(f"ğŸ’¾ ì„±ê³µí•œ ë°ì´í„°ëŠ” gs://sprintda05_final_project/processed/ ì— ì €ì¥ë¨")
    
    # ìµœì¢… ë¦¬ì†ŒìŠ¤ ìƒíƒœ
    try:
        final_resources = check_system_resources()
        logger.info(f"ğŸ”§ ë¦¬ì†ŒìŠ¤ ë³€í™”: ë©”ëª¨ë¦¬ {initial_resources['memory']:.1f}% â†’ {final_resources['memory']:.1f}%")
    except:
        pass
    
    logger.info(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰')
    parser.add_argument('--parallel', action='store_true', help='ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ (ë©”ëª¨ë¦¬ ì¶©ë¶„í•  ë•Œë§Œ)')
    parser.add_argument('--table', type=str, help='íŠ¹ì • í…Œì´ë¸”ë§Œ ì²˜ë¦¬ (accounts_user, hackle_events, accounts_userquestionrecord, accounts_blockrecord)')
    
    args = parser.parse_args()
    
    if args.table:
        # íŠ¹ì • í…Œì´ë¸”ë§Œ ì²˜ë¦¬
        table_map = {
            'accounts_user': ('ì²œì§€í˜„', 'votes', preprocess_accounts_user),
            'hackle_events': ('ì¡°ìˆ˜ì§„', 'hackle', preprocess_hackle_events),
            'accounts_userquestionrecord': ('ì§„ìš°í˜•', 'votes', preprocess_userquestionrecord),
            'accounts_blockrecord': ('ì´ì¤€í¬', 'votes', preprocess_blockrecord)
        }
        
        if args.table in table_map:
            processor, dataset, func = table_map[args.table]
            result = run_single_preprocessing(processor, args.table, dataset, func)
            print(f"\nê²°ê³¼: {result}")
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {args.table}")
            print(f"ì§€ì› í…Œì´ë¸”: {', '.join(table_map.keys())}")
    else:
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        results = run_all_preprocessing(parallel=args.parallel)