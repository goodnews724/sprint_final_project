"""
ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
ë‹´ë‹¹: ê¹€ì¬ë¬¸
"""

import pandas as pd
import os
import logging
from google.cloud import storage
from google.oauth2 import service_account

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def setup_gcs_auth():
    """GCS ì¸ì¦ ì„¤ì •"""
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œ í™•ì¸
        cred_path = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')
        if cred_path and os.path.exists(cred_path):
            logger.info("âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ GCS ì¸ì¦ ì •ë³´ í™•ì¸")
            return
        
        # Airflow Connectionì—ì„œ ì¸ì¦ ì •ë³´ ì°¾ê¸° (ì„ íƒì‚¬í•­)
        # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”
        logger.warning("âš ï¸ GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        logger.info("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì¸ì¦ ì„¤ì •:")
        logger.info("   export GOOGLE_APPLICATION_CREDENTIALS='path/to/service-account-key.json'")
        
    except Exception as e:
        logger.error(f"âŒ GCS ì¸ì¦ ì„¤ì • ì‹¤íŒ¨: {e}")
        raise

def load_table(table_name: str, dataset: str = 'votes') -> pd.DataFrame:
    """GCSì—ì„œ í…Œì´ë¸” ë¡œë“œ (ê°œì„  ë²„ì „)"""
    
    setup_gcs_auth()
    
    # GCS ê²½ë¡œ
    bucket_name = 'sprintda05_final_project'
    blob_path = f"{dataset}/{table_name}.parquet"
    gcs_path = f"gs://{bucket_name}/{blob_path}"
    
    try:
        logger.info(f"ğŸ“¥ {table_name} ë¡œë“œ ì‹œì‘...")
        logger.info(f"   ê²½ë¡œ: {gcs_path}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        import psutil
        memory_before = psutil.virtual_memory().percent
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (ë¡œë“œ ì „): {memory_before:.1f}%")
        
        if memory_before > 85:
            raise MemoryError(f"ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜: {memory_before:.1f}% ì‚¬ìš© ì¤‘")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_path)
        
        if not blob.exists():
            raise FileNotFoundError(f"GCS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gcs_path}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        blob.reload()
        file_size_mb = blob.size / 1024**2
        logger.info(f"   íŒŒì¼ í¬ê¸°: {file_size_mb:.1f}MB")
        
        if file_size_mb > 1000:  # 1GB ì´ìƒì´ë©´ ê²½ê³ 
            logger.warning(f"âš ï¸ í° íŒŒì¼ ê°ì§€: {file_size_mb:.1f}MB - ë¡œë”©ì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # pandasë¡œ ì§ì ‘ ë¡œë“œ (gcsfs ì‚¬ìš©)
        try:
            df = pd.read_parquet(gcs_path, engine='pyarrow')
        except ImportError:
            # gcsfsê°€ ì—†ìœ¼ë©´ ë¡œì»¬ ë‹¤ìš´ë¡œë“œ í›„ ë¡œë“œ
            logger.info("   gcsfs ë¯¸ì„¤ì¹˜ - ë¡œì»¬ ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì‚¬ìš©")
            local_path = f"/tmp/{table_name}_{dataset}_{os.getpid()}.parquet"
            
            try:
                blob.download_to_filename(local_path)
                df = pd.read_parquet(local_path, engine='pyarrow')
            finally:
                if os.path.exists(local_path):
                    os.remove(local_path)
        
        memory_after = psutil.virtual_memory().percent
        logger.info(f"âœ… {table_name} ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,}í–‰, {df.shape[1]}ì—´")
        logger.info(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (ë¡œë“œ í›„): {memory_after:.1f}%")
        logger.info(f"   ë©”ëª¨ë¦¬ ì¦ê°€: +{memory_after - memory_before:.1f}%")
        
        return df
        
    except FileNotFoundError as e:
        logger.error(f"âŒ íŒŒì¼ ì—†ìŒ: {e}")
        raise
    except MemoryError as e:
        logger.error(f"âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
        raise
    except Exception as e:
        logger.error(f"âŒ {table_name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    try:
        df = load_table('accounts_user', 'votes')
        print(f"í…ŒìŠ¤íŠ¸ ì„±ê³µ: {len(df):,}í–‰ ë¡œë“œë¨")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")